CLARITY Digital Twin Health Platform – Current Capabilities and Roadmap

CLARITY is a backend platform that transforms raw wearable data (e.g., from Apple Watch) into clinically meaningful health insights. It combines multi-sensor data processing with machine learning to create a “digital twin” of an individual’s health, focusing on sleep patterns, daily activity, cardiovascular status, and circadian rhythms. Below we detail the system’s current functionality – what the codebase actually does today – and outline a roadmap of features under development. This ensures transparency about which analytical components are fully implemented versus those that are still experimental.

Current Functionality and Components

Pretrained Actigraphy Transformer (PAT) – Actigraphy-Based Sleep & Mood Analysis

The PAT model is a foundation AI model that analyzes wrist motion (actigraphy) data to infer sleep-related metrics and behavioral patterns ￼. Given minute-level step counts or accelerometer data converted into an actigraphy signal, PAT produces:
	•	Sleep Quality Metrics: an 8-element vector (0–1 scaled) estimating key sleep characteristics (e.g. efficiency, latency, fragmentation) from one week of actigraphy ￼. These metrics serve as a proxy for sleep patterns when only movement data is available.
	•	Circadian Rhythm Regularity Score: a single score indicating the consistency of the user’s daily activity/sleep cycle ￼. A higher score suggests a strong day-night pattern, whereas a low score flags irregular sleep-wake timing.
	•	Depression Risk Indicator: a scalar output reflecting patterns in activity that correlate with elevated depression risk ￼. (This is derived from the Dartmouth-trained model, which learned associations between actigraphy patterns and mental health outcomes.)
	•	Activity Embedding: a 96-dimensional latent vector representing the user’s weekly activity profile ￼ ￼, padded to 128-dimensions for downstream use ￼. This embedding can feed into further models or analyses (e.g. for anomaly detection or personalized insights).

How it works: PAT is a Transformer-based model pretrained on data from 29,000+ individuals, making it adept at recognizing sleep/activity patterns from motion data ￼. When a user’s step count series is fed in, the model’s output includes the above metrics. These outputs allow the platform to estimate sleep efficiency and circadian regularity purely from watch data, enabling basic sleep assessments even if dedicated sleep logs are absent. Importantly, PAT is currently limited to aggregate metrics – for example, it does not yet provide minute-by-minute sleep stage labeling. (In fact, the code uses a placeholder for detailed sleep stage output ￼.) Instead, PAT’s value today lies in its summary indices (sleep quality, rhythm consistency, etc.) and the depression-risk proxy, which together augment psychiatric evaluation from everyday activity patterns.

Sleep Processor – Clinical Sleep Metrics from Logged Data

In addition to PAT, the system includes a Sleep Processor that analyzes structured sleep records (e.g. from Apple HealthKit’s sleep tracking). This module computes research-grade sleep features aligned with clinical standards ￼. From nightly sleep logs (bedtime, wake time, durations in each stage if available), it extracts:
	•	Sleep efficiency: the fraction of time in bed spent sleeping ￼.
	•	Sleep latency: how long it took to fall asleep each night ￼.
	•	WASO: minutes awake after sleep onset (a measure of sleep fragmentation) ￼.
	•	Sleep stages percentages: proportions of REM sleep and deep sleep, if stage data is provided ￼.
	•	Awakenings count: number of awakenings during the sleep period.
	•	Sleep consistency: a score reflecting consistency of sleep timing across nights (variability in sleep and wake times).
	•	Overall sleep quality score: an aggregate 0–5 score computed from the above metrics (higher is better).

These features are computed using accepted formulas and thresholds from clinical sleep medicine (AASM guidelines, etc.) ￼. For example, efficiency and latency are graded as “excellent/good/fair/poor” based on cut-offs, and the processor combines multiple factors into an overall quality index. All of this is done on the actual sleep diary data (as opposed to PAT’s inference from motion). The Sleep Processor is fully implemented and active – whenever the platform has explicit sleep tracking data, it will output these detailed metrics. This allows clinicians to see classical sleep measures (e.g. 85% efficiency, 20 min latency) directly computed from the user’s HealthKit logs, providing a familiar, transparent assessment of sleep health alongside PAT’s more novel metrics.

Cardiovascular Processor – Heart Rate & HRV Feature Extraction

The CardioProcessor module derives cardiovascular health indicators from heart rate and heart rate variability (HRV) time series ￼. It ingests timestamped HR readings (and optional HRV data) and outputs a set of 8 features characterizing the user’s cardiovascular profile:
	•	Average and Max Heart Rate: overall mean heart rate and peak heart rate over the period ￼, reflecting baseline cardiovascular load and capacity.
	•	Resting Heart Rate: an estimate of resting HR, typically the 10th percentile of the heart rate distribution during the day ￼. This correlates with fitness (lower resting HR often means better cardiovascular fitness).
	•	Heart Rate Variability (HRV): measured as the standard deviation of inter-beat intervals (or SDNN) if available ￼. HRV is a proxy for autonomic nervous system balance and stress.
	•	HRV Variability: the variability (standard deviation) in the HRV signal itself ￼, indicating consistency of heart rhythm stability.
	•	Heart Rate Recovery Score: a 0–1 index gauging how well the heart rate returns to low levels after exertion ￼ ￼. The algorithm compares the amount of time spent in low HR (resting_threshold, e.g. bottom 25% values) versus high HR (top 25%) periods; a higher ratio of resting to elevated periods yields a higher recovery score, indicating good cardiovascular recovery capacity ￼ ￼.
	•	Circadian Rhythm Score (Cardio): a 0–1 score assessing day–night heart rate variation ￼ ￼. The processor evaluates whether the heart rate shows healthy dips at night: it groups HR by hour of day and checks that average nighttime HR is lower than daytime HR. A strong day-night difference yields a higher circadian rhythm regularity score ￼ ￼.

These cardiovascular features are fully implemented and computed whenever heart rate data is available. They provide clinicians and users with interpretable heart metrics: for example, one can track improvements in resting HR or see if lack of nighttime HR dip might indicate stress or overtraining. (At present, the code focuses on summary statistics and regularity; it does not perform arrhythmia detection or complex ECG waveform analysis – those are on the roadmap, as noted later.) Nonetheless, the cardiovascular module gives a solid snapshot of heart health from wearable data, including novel insights like a circadian HR pattern metric.

Respiratory Processor – Breathing Rate and Oxygen Saturation Analysis

The RespirationProcessor handles data from breathing (respiratory rate) and blood oxygen saturation (SpO₂) sensors ￼. From these time-series it computes another 8-feature vector characterizing respiratory health:
	•	Average Respiratory Rate: mean breathing rate (breaths per minute) over the logged period ￼.
	•	Resting Respiratory Rate: an estimate of the baseline resting breath rate (e.g. the lower quartile of the RR distribution) – a lower value may indicate better cardiorespiratory fitness when at rest.
	•	Respiratory Variability: variability (standard deviation) in respiratory rate ￼, reflecting how steady or irregular the breathing pace is.
	•	Average SpO₂: mean blood oxygen saturation (%) during the period ￼. Typically high 90s% for healthy individuals; lower averages could indicate chronic issues.
	•	Minimum SpO₂: the lowest oxygen saturation observed ￼ (useful to catch any drops during sleep or exercise that might be of concern, e.g. if falling into low 90s or below).
	•	SpO₂ Variability: variability in oxygen saturation readings ￼. Large variability might suggest measurement noise or sporadic desaturation events.
	•	Respiratory Stability Score: a 0–1 index of how stable the breathing rate is over time ￼ ￼. The processor looks at short-term fluctuations; a score of 1 would mean very steady breathing, while lower values mean frequent changes in rate.
	•	Oxygenation Efficiency Score: a 0–1 score indicating effective oxygenation ￼. This metric rewards consistently high SpO₂ levels; if oxygen saturation is generally well-maintained (with only minor drops), the score will be high. It’s essentially an indicator of whether the person’s blood oxygen remained in a healthy range.

These respiratory features are computed whenever wearable devices provide breathing and pulse-oximeter data (some advanced wearables or devices during sleep can measure these). Currently, the respiratory module is fully implemented and active, though it assumes data is present – in practice, continuous respiratory data might be less commonly available than heart or step data. The presence of this module future-proofs the platform for integrating devices like O₂ monitors or advanced fitness trackers. Clinicians could use these outputs to screen for issues like nocturnal breathing irregularities (low stability or efficiency might hint at sleep apnea or COPD) or to track respiratory fitness over time.

Activity Processor – Daily Activity & Fitness Summary

While PAT provides deep analysis of movement patterns, CLARITY also calculates straightforward activity statistics for transparency. The ActivityProcessor ingests raw activity metrics from Apple Health (steps, distance, exercise minutes, etc.) and produces user-friendly summaries ￼. It yields 12 key activity metrics:
	1.	Total Steps over the period ￼ (e.g. total steps in a week).
	2.	Average Daily Steps ￼.
	3.	Peak Daily Steps (the highest single-day step count) – indicates the maximum activity level achieved.
	4.	Total Distance walked/run (in kilometers) ￼.
	5.	Average Daily Distance.
	6.	Total Active Energy burned (kilocalories) ￼.
	7.	Average Daily Active Energy.
	8.	Total Exercise Time (minutes of workouts or “exercise” as defined by Apple Watch) ￼.
	9.	Average Daily Exercise Minutes.
	10.	Total Flights Climbed (stairs).
	11.	Total Active Minutes (sum of all moderate/high activity minutes).
	12.	Activity Consistency Score (0–1): a custom index of how consistent the user’s activity is day-to-day ￼. If the user has regular activity patterns (not too many zero-step days followed by ultra-active days), this score will be higher.

All these metrics are derived from the user’s raw activity logs with minimal inference – they answer basic questions like “How many steps did I walk this week?” or “Am I meeting my daily exercise goals on average?” The ActivityProcessor complements the PAT model’s complex analytics by providing transparent, easily explainable numbers ￼. This dual approach (basic summaries + advanced AI) helps build trust: a clinician or user can validate the AI’s conclusions against the raw totals (e.g. if PAT says “circadian rhythm irregular,” one might also notice the step counts are highly inconsistent across days). The activity metrics are fully functional and are stored as part of each analysis result.

Fusion Transformer – Multi-Modal Health State Integration

A distinctive feature of the CLARITY backend is the FusionTransformer: a PyTorch-based model that fuses multiple modality features into one unified representation ￼. If a user has data in more than one domain (say sleep + cardio, or activity + heart + respiratory), the pipeline invokes this fusion step. The FusionTransformer uses a small transformer encoder to integrate the modalities: it projects each modality’s feature vector into a common embedding space, concatenates them (with a special [CLS] token), and applies self-attention to learn cross-modal interactions ￼ ￼. The output is a single 64-dimensional vector that summarizes the combined health data ￼ ￼. In essence, this fused health vector is an encoded snapshot of the person’s overall physiological state, taking into account all available streams.

Current use: The FusionTransformer is implemented and invoked whenever multiple modalities are present. For example, if a user has sleep, activity, and heart data, the system will generate each modality’s features, then feed them into the fusion model to produce one 64-D vector ￼ ￼. This vector is then stored as part of the analysis results ￼ ￼. At this stage, the fused vector is primarily used for research and future development – it’s not yet directly exposed as a standalone insight to end users. However, logging confirms that the model does execute (e.g. “Fused 3 modalities into 64-dim vector” is recorded in the logs when applicable) ￼. The presence of this fusion step is forward-looking: it lays the groundwork for advanced multi-modal analyses (such as detecting complex patterns that only emerge when correlating heart, sleep, and activity together). For now, consider the fused vector as the “digital twin fingerprint” of a user’s health in a given time window – a concise representation that future algorithms or clinicians could examine for integrated anomalies or improvements.

Summary Statistics and Insights Generation

After computing all the above, the pipeline also produces summary statistics and preliminary interpretations. This includes data coverage info (how many data points, over how many hours) and simple statistical summaries for each feature vector ￼ ￼. Moreover, the platform computes health indicator flags – for instance, it uses thresholds to label cardiovascular fitness as “good/fair/poor” based on the resting HR and recovery scores, or to rate sleep quality as excellent/good/etc. via the Sleep Processor’s summary ￼ ￼. These indicators provide at-a-glance context (e.g. “respiratory health: moderate – average SpO₂ 96%”). All results, including raw features and any ratings, are saved to the database (Firestore) for downstream use ￼ ￼.

Finally, CLARITY integrates with a natural language model (Google’s Gemini, as referenced in documentation) to turn these metrics into human-readable advice. For example, given the analysis results, a user or clinician can query in plain English (“How is my sleep affecting my health?”) and the system will generate a conversational answer drawing on the computed metrics. This AI-driven insight generation is built on top of the analytics pipeline (the demo uses a /insights/generate API that takes the numeric results and returns a text explanation). While the details of the LLM integration are beyond this codebase analysis, it’s worth noting that all the computed health features can be contextualized for end-users via an NLP layer – bridging the gap between raw data and understanding. This is especially useful for investors or non-technical stakeholders: the platform not only computes data but also explains it in accessible language (e.g. “Your sleep efficiency was 85% which is good, but your circadian rhythm score was a bit low, suggesting your sleep schedule varies – try to keep a more consistent bedtime.”).

In summary, the current CLARITY backend is fully capable of ingesting wearable data and outputting a rich set of health metrics. The Pretrained Actigraphy Transformer provides advanced analysis of movement patterns (yielding sleep and mood-related indicators), and is complemented by dedicated processors for sleep logs, heart data, respiratory data, and activity summaries. All these components are active in the codebase today – they run in an asynchronous pipeline to produce an integrated health report for each data upload. The claims and functionality described above are backed directly by the implemented code (as cited), ensuring that our description reflects the true state of the system rather than aspirational plans.

Roadmap – Upcoming and In-Progress Features

While the platform’s core analytics are in place, some higher-level features remain under development or planned for future releases. These upcoming enhancements will expand the medical and data science utility of CLARITY beyond its current capabilities:
	•	Fine-Grained Sleep Stage & Apnea Detection: Improving the PAT model or related algorithms to perform detailed sleep staging (light, deep, REM) from actigraphy and other signals. Currently, PAT infers overall sleep quality but uses a placeholder for exact sleep stage timelines ￼. A future update will integrate more data (possibly heart rate or breathing patterns during sleep) to identify sleep stages more accurately, and potentially flag signs of sleep apnea or insomnia patterns (e.g. frequent awakenings) in a clinically validated manner. This will move the sleep analysis from “general metrics” to “specific clinically actionable events.”
	•	Arrhythmia and Cardiac Anomaly Detection: Building on the heart rate processing, the team plans to add detection of irregular heart rhythms (such as atrial fibrillation or extrasystoles) from wearables. This may involve analyzing heart rate time-series for irregular variability or integrating smartwatch ECG data if available. Presently, the CardioProcessor does not analyze beat-to-beat variability for arrhythmias – it focuses on averages and variability. A future module or an extension of the current one will perform real-time arrhythmia screening, alerting if the pattern of HR/HRV suggests possible atrial fibrillation or other cardiac events. This feature is important for transitioning the platform from wellness monitoring to clinical-grade alerting.
	•	Multi-Modal Risk Prediction & Alerts: Leveraging the FusionTransformer output to predict composite health outcomes. With multiple modalities fused into a 64-D health state vector, the next step is to train predictive models that use this vector to foresee risks (e.g. flagging if a combination of poor sleep + high resting HR + low activity might predict an impending depressive episode or cardiac strain). These would manifest as alerts or risk scores for users and clinicians. For example, a “Health Stability Index” could be derived from the fused data – not yet implemented, but the infrastructure is ready for it. This will fully capitalize on the multi-modal nature of the data, moving from descriptive analytics to predictive analytics.
	•	Circadian Rhythm Optimization Recommendations: Going beyond scoring circadian regularity, the platform will offer personalized guidance to improve it ￼. For instance, if the system detects an irregular sleep schedule (low circadian rhythm score), it could recommend gradually aligning bedtime and wake-up time or increasing morning light exposure. These recommendations would be generated by analyzing patterns over weeks and comparing them to desired stable patterns. This feature extends the current analytics (which flag a problem) with actionable advice to fix the problem – crucial for user engagement and clinical impact.
	•	Longitudinal Trend Analysis: Implementing views and metrics that track changes in the user’s health metrics over longer periods (months, years) ￼. The current system analyzes each batch of uploaded data in isolation (typically on a weekly basis). A planned enhancement is to compute trends, such as improvements or deteriorations in sleep quality, resting heart rate, activity levels, etc. For example, the system could report, “Your average daily steps increased by 20% this month compared to last month” or “Your sleep efficiency has been declining over the past 3 months.” By detecting trends, CLARITY can serve as a long-term health monitor, not just a point-in-time analyzer. Implementing this will likely involve aggregating the summary stats stored in Firestore across time and possibly using anomaly detection to spot significant deviations.
	•	Personalized Intervention & Coaching: Using the insights (and possibly the language generation capabilities) to provide tailored health coaching ￼. This goes hand-in-hand with the recommendations and trends above. The idea is to translate data into a health improvement plan – for instance, if the system knows a user’s cardiovascular fitness is suboptimal and activity is low, it might suggest a walking program; if sleep consistency is poor, it might suggest a nighttime routine. These interventions would be based on rules or AI models that map patterns to advice, developed in consultation with clinical guidelines. While the current platform can identify issues, the future goal is for it to suggest solutions, effectively closing the loop from data to action.
	•	Expanded Data Source Integration: On the horizon is support for additional wearable data streams beyond the current Apple HealthKit scope. This includes incorporating data from new sensors or external databases – for example, continuous glucose monitors, blood pressure cuffs, or stress (EDA) sensors, as they become available. The architecture is designed to be extensible; new modalities could be plugged in with their own processor and added to the fusion model’s inputs. One specific planned extension is integrating mood and context data (such as patient-reported outcomes or questionnaires). Since CLARITY is targeted at mental health, combining subjective mood reports with objective sensor data could improve models like the depression risk predictor. These integrations are not active yet but are conceptually mapped out in the project’s plans.
	•	Quality Improvements & Validation: Although not a “feature” per se, an ongoing roadmap item is rigorous validation of the algorithms (comparing PAT’s inferred metrics against clinical polysomnography for sleep, validating the HR recovery score against fitness tests, etc.) and improving the model performance. The current codebase has extensive tests (700+ unit tests) ensuring functionality, but clinical validation studies will be important as the product moves toward healthcare deployment. Additionally, expect improvements in test coverage and error handling as noted in development plans (aiming for >85% coverage from the current ~59%, per project updates).

All future features will be implemented with an eye toward maintaining medical accuracy and transparency. For example, any arrhythmia detection algorithm will likely include explainability (e.g. highlighting the section of tachogram that triggered the alert), and any personalized recommendation will cite the metric that prompted it (e.g. “Your average HRV of 20 ms is below normal, so consider stress reduction techniques.”). The roadmap aligns with the vision of a comprehensive digital health companion that not only tracks and predicts health changes but also guides the user or clinician in responding to those changes ￼.

⸻

Alignment with Existing Documentation (README Changes)

(The following points highlight discrepancies between previous documentation and the actual code, with changes needed to ensure accuracy in the project README.)
	•	Clarify PAT’s Role: The README should clarify that the Pretrained Actigraphy Transformer analyzes wrist movement data to infer sleep and circadian patterns, rather than implying it directly measures sleep stages. Any statement suggesting that PAT performs full “sleep stage detection” should be tempered – in reality, PAT outputs aggregate sleep metrics and a circadian score, not per-epoch sleep stage classifications (the code even uses a placeholder for detailed stages) ￼. The README should highlight PAT’s validated outputs (sleep efficiency, etc.) from actigraphy, and note that it provides a depression risk score from activity patterns – a unique but currently implemented feature. This ensures we don’t overstate PAT as doing more than it actually does today.
	•	Include the New Sleep Processor: The existing docs focused heavily on PAT and may have treated the Sleep Processor as a future or minor component. We must update the README to reflect that a dedicated Sleep Processor module is implemented and active. It should detail how we use actual sleep logs to calculate clinical sleep measures (efficiency, latency, REM%, WASO, etc.) ￼. This addition corrects any previous omission and shows that the platform can analyze both actigraphy-derived sleep estimates and real sleep diary data in parallel, a capability now realized in code.
	•	Highlight Cardiovascular and Respiratory Analytics: If the current README does not mention the cardio and respiratory processors (or only listed them as theoretical), it needs to be updated. We should explicitly describe that heart rate and HRV data are processed into 8 cardiovascular features (average HR, resting HR, HRV, recovery score, circadian rhythm of HR, etc.) ￼, and that respiratory rate and SpO₂ are processed into 8 respiratory features (average RR, SpO₂ levels, stability scores) ￼. This correction is important so that readers (especially clinicians or investors) understand that the platform already covers these vital health domains, not just sleep. Any previous implication that these were only envisioned should be replaced with the fact that they are implemented in the current codebase.
	•	State the Functionality of Multi-Modal Fusion Realistically: The README should mention the Fusion Transformer as an implemented component that creates an integrated health vector ￼, but also clarify its current usage. If previously the documentation suggested that multi-modal fusion was providing combined insights or diagnoses, we need to adjust that: the fusion model is in place and fusing data (for future use) ￼, but its output is not yet driving specific user-facing conclusions. In other words, we should present it as infrastructure ready for advanced analytics rather than claiming it already delivers new health scores. This nuance will prevent over-promising on what the multi-modal analysis is currently delivering.
	•	Correct Any Inflated Claims: During development, some documents (e.g. older audits) contained incorrect technical claims, such as overstating PAT’s capabilities or marking planned features as done. For example, if the README or prior audit said “PAT achieves sleep staging accuracy” or “arrhythmia detection is included,” those statements must be corrected to reflect the true status (PAT provides sleep quality metrics, and arrhythmia detection is planned, not implemented). The updated README should rely on factual, cited descriptions like “sleep stages are currently placeholders in the PAT output” ￼ to ensure honesty. Similarly, any reference to “Gemini AI providing insights” should clarify whether that integration is experimental or in use, so as not to confuse the reader about which AI components are operational.
	•	Add a Roadmap Section: If the existing README lacks forward-looking context, we should incorporate a “Roadmap” (as done above) to transparently list features under development – multi-modal predictive analytics, arrhythmia alerts, more personalized recommendations, etc. This sets investor and user expectations correctly: they can see which exciting features are on the horizon without assuming they’re available today. For instance, explicitly mentioning “arrhythmia detection” and “longitudinal trend analysis” as planned (and not yet active) will align the documentation with reality and the development plan ￼. Each roadmap item can be tied to the relevant component (e.g., arrhythmia ties to CardioProcessor enhancements, sleep staging ties to PAT improvements) to make it clear how current and future development map together.

By implementing the above changes, the project README will accurately reflect the true state of the codebase – showcasing the robust functionality already in place, while honestly delineating which features are still being worked on. This accuracy is crucial for maintaining credibility with all stakeholders, ensuring that clinicians, developers, and investors all understand what CLARITY can do today and what it will offer tomorrow. The end result is a README that is truthful, medically grounded, and aligned with the actual code, supported by inline citations from the code where appropriate.