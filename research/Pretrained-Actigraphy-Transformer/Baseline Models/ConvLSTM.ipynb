{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DLnvDDAvZcg"
   },
   "source": [
    "# Baseline Test - ConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luetswgMv_8y"
   },
   "source": [
    "Trained with Google TPU v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpupLYYoxvNH"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hgz3qjavw6xX"
   },
   "outputs": [],
   "source": [
    "# write where you want to save all your files\n",
    "folder_root = \"/content/drive/MyDrive/ActigraphyTransformer/A-NEW/Baseline Tests\"\n",
    "folder_Data_2013 = (\n",
    "    \"/content/drive/MyDrive/ActigraphyTransformer/A-NEW/Baseline Tests/Data_2013\"\n",
    ")\n",
    "folder_ConvLSTM = (\n",
    "    \"/content/drive/MyDrive/ActigraphyTransformer/A-NEW/Baseline Tests/ConvLSTM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSdGffCewcSF"
   },
   "source": [
    "# Imports and **Connect To TPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "PSeNwoDSEHXt"
   },
   "outputs": [],
   "source": [
    "!pip install pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Gs2bFVn5ZMb"
   },
   "outputs": [],
   "source": [
    "# @title Importing\n",
    "\n",
    "# Packages\n",
    "import os\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import Layers\n",
    "from keras.layers import (\n",
    "    LSTM,\n",
    "    Activation,\n",
    "    Conv1D,\n",
    "    ConvLSTM2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPooling1D,\n",
    "    MaxPooling3D,\n",
    "    TimeDistributed,\n",
    ")\n",
    "\n",
    "# from keras.layers.embeddings import Embedding\n",
    "from keras.metrics import AUC\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afsH2qmUxGQi"
   },
   "outputs": [],
   "source": [
    "# SEEDS\n",
    "\n",
    "# Hard Code Random Seeds.\n",
    "r1 = 0\n",
    "r2 = 1\n",
    "\n",
    "# Set Random Seed\n",
    "random.seed(r1)\n",
    "tf.random.set_seed(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "N4rQXYZcwz73"
   },
   "outputs": [],
   "source": [
    "# @title Connect to TPU\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Connect to the TPU cluster or fall back to CPU/GPU\n",
    "try:\n",
    "    resolver = (\n",
    "        tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    )  # Tries to connect to the TPU\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    devices = tf.config.list_logical_devices(\"TPU\")\n",
    "    print(\"TPU devices:\", devices)\n",
    "except ValueError:\n",
    "    print(\"Could not connect to TPU; using CPU/GPU strategy instead.\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Example computation using the strategy\n",
    "with strategy.scope():\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "    @tf.function\n",
    "    def matmul_fn(x, y):\n",
    "        return tf.matmul(x, y)\n",
    "\n",
    "    z = strategy.run(matmul_fn, args=(a, b))\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye8xyruzvf68"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ipXbzcRdJH7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Please Fill out Parameters Below\n",
    "\"\"\"\n",
    "\n",
    "Smoothing = False\n",
    "\n",
    "Task = \"SSRI\"\n",
    "\n",
    "Tasks = [\n",
    "    \"SSRI\",\n",
    "    \"Benzodiazepine\",\n",
    "    \"Sleep Strict\",\n",
    "    \"Sleep Liberal\",\n",
    "    \"Depression\",\n",
    "]  # pick a task from Tasks and set the \"Task\" variable in the above line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhCv9I8wnrxk"
   },
   "outputs": [],
   "source": [
    "if Smoothing:\n",
    "    mode = \"Smooth\"\n",
    "else:\n",
    "    mode = \"Raw\"\n",
    "\n",
    "\n",
    "if \"Depression\" in condition:\n",
    "    train_sizes = [100, 250, 500, 1000, 2500, 2800]\n",
    "    data_folder_location = os.path.join(\n",
    "        folder_Data_2013, f\"All_Meds_Depression/{mode}/TestSize2000_set1\"\n",
    "    )\n",
    "\n",
    "elif \"Strict\" in condition:\n",
    "    train_sizes = [100, 250, 500, 1000, 2500, 3429]\n",
    "    data_folder_location = os.path.join(\n",
    "        folder_Data_2013, f\"All_Meds_SleepDisorder_Strict/{mode}/TestSize2000_set1\"\n",
    "    )\n",
    "\n",
    "elif \"Liberal\" in condition:\n",
    "    train_sizes = [100, 250, 500, 1000, 2500, 3429]\n",
    "    data_folder_location = os.path.join(\n",
    "        folder_Data_2013, f\"All_Meds_SleepDisorder_Liberal/{mode}/TestSize2000_set1\"\n",
    "    )\n",
    "\n",
    "elif \"Benzos\" in condition:\n",
    "    train_sizes = [100, 250, 500, 1000, 2500, 5769]\n",
    "    data_folder_location = os.path.join(\n",
    "        folder_Data_2013, f\"All_Meds/{mode}/TestSize2000_set1\"\n",
    "    )\n",
    "\n",
    "elif \"SSRI\" in condition:\n",
    "    train_sizes = [100, 250, 500, 1000, 2500, 5769]\n",
    "    data_folder_location = os.path.join(\n",
    "        folder_Data_2013, f\"All_Meds_Taking_SSRI/{mode}/TestSize2000_set1\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid condition\")\n",
    "\n",
    "print(f\"Current train sizes: {train_sizes}\")\n",
    "print(f\"Data located at {data_folder_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLKwPPVLddSA"
   },
   "outputs": [],
   "source": [
    "test_size = 2000  # fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDGqy1eCdexA"
   },
   "outputs": [],
   "source": [
    "# first save the test sets\n",
    "X_test = np.load(os.path.join(data_folder_location, f\"X_test_{test_size}.npy\"))\n",
    "y_test = np.load(os.path.join(data_folder_location, f\"y_test_{test_size}.npy\"))\n",
    "\n",
    "# standard scalar on X test\n",
    "train_scalar = StandardScaler()\n",
    "train_scalar.fit(X_test)\n",
    "X_test = train_scalar.transform(X_test)\n",
    "\n",
    "\n",
    "n_participants_test = X_test.shape[0]\n",
    "n_steps, n_length, n_width = 7, 24, 60\n",
    "n_features = 1\n",
    "X_test = X_test.reshape((X_test.shape[0], n_steps, n_length, n_width, n_features))\n",
    "\n",
    "print(\"successfully loaded X test and y test\")\n",
    "print(f\"Shape of X test: {X_test.shape}\")\n",
    "print(f\"Shape of y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfcwVnOZdkdR"
   },
   "source": [
    "Then, load the train and validation datasets by saving them into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aEDkEHzdjh4"
   },
   "outputs": [],
   "source": [
    "train_sets = {}\n",
    "val_sets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVn-5Gy4dnis"
   },
   "outputs": [],
   "source": [
    "for size in train_sizes:\n",
    "    # X train, X val original\n",
    "    X_train = np.load(os.path.join(data_folder_location, f\"X_train_{size}.npy\"))\n",
    "    X_val = np.load(os.path.join(data_folder_location, f\"X_val_{size}.npy\"))\n",
    "\n",
    "    # apply standard scalar\n",
    "    train_scalar = StandardScaler()\n",
    "    train_scalar.fit(X_train)\n",
    "    X_train = train_scalar.transform(X_train)\n",
    "\n",
    "    # Reshape Train, Validation and Test\n",
    "    n_participants_train = X_train.shape[0]\n",
    "    n_participants_val = X_val.shape[0]\n",
    "    n_timesteps = X_train.shape[1]\n",
    "    n_features = 1\n",
    "    X_train = X_train.reshape((\n",
    "        X_train.shape[0],\n",
    "        n_steps,\n",
    "        n_length,\n",
    "        n_width,\n",
    "        n_features,\n",
    "    ))\n",
    "    y_train = np.load(os.path.join(data_folder_location, f\"y_train_{size}.npy\"))\n",
    "\n",
    "    # X val\n",
    "    # apply standard scalar\n",
    "    val_scalar = StandardScaler()\n",
    "    val_scalar.fit(X_val)\n",
    "    X_val = val_scalar.transform(X_val)\n",
    "    X_val = X_val.reshape((X_val.shape[0], n_steps, n_length, n_width, n_features))\n",
    "\n",
    "    y_val = np.load(os.path.join(data_folder_location, f\"y_val_{size}.npy\"))\n",
    "\n",
    "    train_sets[size] = (X_train, y_train)\n",
    "    val_sets[size] = (X_val, y_val)\n",
    "\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(f\"Train set size: {len(train_sets)}\")\n",
    "print(f\"Val set size: {len(val_sets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yvvLm8FdpRG"
   },
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryxt1DjpdrM2"
   },
   "outputs": [],
   "source": [
    "for key, value in train_sets.items():\n",
    "    print(f\"For train size {key}: \")\n",
    "\n",
    "    # print the shapes of X train and y train\n",
    "    print(f\"X train shape: {value[0].shape}\")\n",
    "    print(f\"y train shape: {value[1].shape}\")\n",
    "\n",
    "    # also print the shapes of X val and y val\n",
    "    print(f\"X val shape: {val_sets[key][0].shape}\")\n",
    "    print(f\"y val shape: {val_sets[key][1].shape}\")\n",
    "\n",
    "    print(\"================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj_89bc4yqqi"
   },
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvoWSO8WMGn8"
   },
   "outputs": [],
   "source": [
    "# Model Structure\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # conv Layers\n",
    "    model.add(\n",
    "        ConvLSTM2D(\n",
    "            filters=32,\n",
    "            kernel_size=(5, 5),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(n_steps, n_length, n_width, n_features),\n",
    "            return_sequences=True,\n",
    "        )\n",
    "    )\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(\n",
    "        ConvLSTM2D(\n",
    "            filters=64, kernel_size=(2, 2), padding=\"valid\", return_sequences=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # feed forward Layers\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    )  # Sigmoid b/c our outcome is binary.\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hUHE4Nze9Kb"
   },
   "source": [
    "## Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_7oW1fHe-U8"
   },
   "outputs": [],
   "source": [
    "# Compile the model -----\n",
    "with strategy.scope():\n",
    "    train_model = create_model()\n",
    "    train_model.compile(\n",
    "        # Metrics\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=tf.keras.metrics.AUC(name=\"auc\"),\n",
    "        # Optimizer\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.00001,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-07,\n",
    "            amsgrad=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# save model weights\n",
    "train_model.save_weights(\"original_model_weights.h5\")\n",
    "\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b199IkGy0_C"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuTWUBdVfRL4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling1D,\n",
    "    MaxPooling1D,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UO7Uitf4fM1d"
   },
   "outputs": [],
   "source": [
    "# mechanisms\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss\n",
    "    factor=0.5,  # Reduce rate by a factor of 0.5\n",
    "    patience=250,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-4,  # Minimum learning rate that the reduction can reach\n",
    "    verbose=1,  # Print messages when reducing the learning rate\n",
    ")\n",
    "\n",
    "# earlyStopping callback\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor=\"val_auc\",  # monitor validation AUC\n",
    "    mode=\"max\",  # maximize AUC\n",
    "    patience=250,  # number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,  # display messages when early stopping is triggered\n",
    "    restore_best_weights=True,  # restore model weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKUA0aX2fYlN"
   },
   "source": [
    "## set up weight folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8_GLFmofbD-"
   },
   "outputs": [],
   "source": [
    "# model weights saving path\n",
    "model_weight_path = os.path.join(\n",
    "    folder_ConvLSTM, f\"Model Weights/All_Meds{condition}/{mode}\"\n",
    ")\n",
    "if not os.path.exists(model_weight_path):\n",
    "    os.makedirs(model_weight_path)\n",
    "print(f\"current model weight path: {model_weight_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmP1MR6bfwpJ"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbKQvR_ffyV8"
   },
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSQaWel-fpoc"
   },
   "outputs": [],
   "source": [
    "scores = {}\n",
    "scores[\"test\"] = {}\n",
    "scores[\"val\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvCfk0gAfreU"
   },
   "outputs": [],
   "source": [
    "model_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGYMFGfwzCT4"
   },
   "outputs": [],
   "source": [
    "for size in train_sizes:\n",
    "    print(f\"\\nSIZE:{size}\")\n",
    "\n",
    "    # Load X_train and fit\n",
    "    X_train, y_train = train_sets[size]\n",
    "\n",
    "    # Load X_val and fit\n",
    "    X_val, y_val = val_sets[size]\n",
    "\n",
    "    print(f\"X Train size: {X_train.shape}\")\n",
    "    print(f\"X Val size: {X_val.shape}\")\n",
    "    print(f\"Y Train size: {y_train.shape}\")\n",
    "    print(f\"Y Val size: {y_val.shape}\")\n",
    "\n",
    "    print(\"loaded X train and X val\")\n",
    "\n",
    "    # Set Class Weights = Balance\n",
    "    class1 = sum(y_train)\n",
    "    total = len(y_train)\n",
    "    class0 = total - class1\n",
    "\n",
    "    class_weights = {0: (class1 / total), 1: (class0 / total)}\n",
    "\n",
    "    print(f\"class weights: {class_weights}\")\n",
    "\n",
    "    # reset model weights\n",
    "    train_model.load_weights(\"original_model_weights.h5\")\n",
    "\n",
    "    # get the corresponding model from the model dictionary\n",
    "\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    # Train model\n",
    "    history = train_model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,  # Edit\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        shuffle=False,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[early_stopper, reduce_lr],\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    # save model history\n",
    "    model_histories[size] = history\n",
    "\n",
    "    # Save model\n",
    "    current_model_name = f\"ConvLSTM_{size}.h5\"\n",
    "    current_model_weights_name = f\"ConvLSTM__{size}_weights.h5\"\n",
    "    print(\"current model name: \", current_model_name)\n",
    "    train_model.save(os.path.join(model_weight_path, current_model_name))\n",
    "    train_model.save_weights(\n",
    "        os.path.join(model_weight_path, current_model_weights_name)\n",
    "    )\n",
    "    print(\"model and weights saved\")\n",
    "\n",
    "    # Test model\n",
    "    test_scores = train_model.evaluate(X_test, y_test, batch_size=64)  # Test Set\n",
    "    scores[\"test\"][size] = test_scores[1]\n",
    "    print(\"Test AUC:\", test_scores[1])\n",
    "\n",
    "    val_scores = train_model.evaluate(X_val, y_val, batch_size=64)  # Val Set\n",
    "\n",
    "    scores[\"val\"][size] = val_scores[1]\n",
    "    print(\"Val AUC:\", val_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvhl1OgQzp3f"
   },
   "outputs": [],
   "source": [
    "# let's look at the scores\n",
    "for key, value in scores.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuehQapZhu9q"
   },
   "outputs": [],
   "source": [
    "# Save all results in a .txt\n",
    "print(\"\\n\\n\")\n",
    "print(scores)\n",
    "\n",
    "results_path = os.path.join(folder_ConvLSTM, f\"results{condition}_{mode}.txt\")\n",
    "\n",
    "try:\n",
    "    file_to_write = open(results_path, \"w\")\n",
    "    file_to_write.write(str(scores))\n",
    "    file_to_write.close()\n",
    "    print(\"Successfully wrote to file\")\n",
    "\n",
    "except:\n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWaVmsdLz_Rf"
   },
   "source": [
    "# Model Introspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9iEdU1Y0SDH"
   },
   "source": [
    "AUC OVER EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16kjLJ0iAuxv"
   },
   "outputs": [],
   "source": [
    "model_histories_plot_path = os.path.join(\n",
    "    folder_ConvLSTM, f\"History Plots/All_Meds{condition}/{mode}\"\n",
    ")\n",
    "if not os.path.exists(model_histories_plot_path):\n",
    "    os.makedirs(model_histories_plot_path)\n",
    "print(f\"current model weight path: {model_histories_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldok24FJAvs_"
   },
   "outputs": [],
   "source": [
    "# inspect model histories\n",
    "for key, value in model_histories.items():\n",
    "    print(f\"Model history for train size {key}:\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhMvBnFYAzae"
   },
   "outputs": [],
   "source": [
    "# Save each plot individually\n",
    "for size, history in model_histories.items():\n",
    "    print(f\"Currently plotting graph with size {size}\")\n",
    "\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.title(f\"AUC over Epochs, Train Size {size}\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.plot(history.history[\"val_auc\"], label=\"Validation AUC\")\n",
    "    plt.plot(history.history[\"auc\"], label=\"Train AUC\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(\n",
    "        model_histories_plot_path, f\"AUC_Over_Epochs_TrainSize{size}.png\"\n",
    "    )\n",
    "    plt.savefig(plot_path)  # Saves as PNG\n",
    "    print(f\"Saved figure at {plot_path}\")\n",
    "    plt.close()  # Close the plot to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIiSNlRXA1E9"
   },
   "outputs": [],
   "source": [
    "# save as one big plot\n",
    "\n",
    "num_models = len(model_histories)\n",
    "cols = 3  # Number of columns in subplot grid\n",
    "rows = (num_models + cols - 1) // cols  # Calculate required number of rows\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(14 * cols, 5 * rows))\n",
    "fig.suptitle(\"AUC over Epochs by Train Size\", fontsize=20)\n",
    "\n",
    "for idx, (size, history) in enumerate(model_histories.items()):\n",
    "    ax = axes[idx // cols, idx % cols]\n",
    "    ax.set_title(f\"Train Size {size}\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"AUC\")\n",
    "    ax.plot(history.history[\"val_auc\"], label=\"Validation AUC\")\n",
    "    ax.plot(history.history[\"auc\"], label=\"Train AUC\")\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Save the combined plot\n",
    "plot_path = os.path.join(model_histories_plot_path, \"Combined_AUC_Over_Epochs.png\")\n",
    "plt.savefig(plot_path)\n",
    "print(f\"Saved combined figure at {plot_path}\")\n",
    "plt.close()  # Close the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_OwHOkzA2w2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
